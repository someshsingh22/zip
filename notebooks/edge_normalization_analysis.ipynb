{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Edge Normalization Analysis\n",
        "\n",
        "This notebook loads the graph dataset parquets (posts and comments), constructs the bipartite edges between `user` and `subreddit`, and explores edge weight distributions to propose robust normalization schemes.\n",
        "\n",
        "Goals:\n",
        "- Inspect heavy-tailed activity/popularity for users and subreddits\n",
        "- Compare several edge normalizations beyond simple log scaling\n",
        "- Provide actionable recommendations for training (e.g., link prediction, GNN message passing)\n",
        "\n",
        "Outputs mirror the CLI in `scripts/analyze_edges.py` and include histograms, correlations, and quantiles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports & setup\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure local src is importable\n",
        "sys.path.append(os.path.abspath(\"../src\"))\n",
        "\n",
        "from core.edge_analysis import (\n",
        "    build_combined_edges,\n",
        "    compute_marginals,\n",
        "    compute_edge_normalizations,\n",
        "    gather_edge_statistics,\n",
        "    summarize_quantiles,\n",
        "    compute_correlations,\n",
        ")\n",
        "\n",
        "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.width\", 140)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config\n",
        "POSTS_FILE = \"../data/merged_submissions_filtered_gt1_dsp.parquet\"\n",
        "COMMENTS_FILE = \"../data/merged_comments_filtered_3x3_dsp.parquet\"\n",
        "BM25_K1 = 1.2\n",
        "BM25_B = 0.75\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load parquets\n",
        "print(\"Loading Parquet files...\")\n",
        "posts_df = pl.read_parquet(POSTS_FILE)\n",
        "comments_df = pl.read_parquet(COMMENTS_FILE)\n",
        "posts_df.head(), comments_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build combined edges\n",
        "edges = build_combined_edges(posts_df, comments_df)\n",
        "user_stats, sub_stats = compute_marginals(edges)\n",
        "stats = gather_edge_statistics(edges, user_stats, sub_stats)\n",
        "print(\n",
        "    f\"Users: {stats.num_users:,} | Subs: {stats.num_subreddits:,} | Edges: {stats.num_edges:,} | Total: {stats.total_interactions:,}\"\n",
        ")\n",
        "edges.head(), user_stats.head(), sub_stats.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute normalization schemes\n",
        "enriched = compute_edge_normalizations(edges, user_stats, sub_stats, bm25_k1=BM25_K1, bm25_b=BM25_B)\n",
        "metrics = [\n",
        "    \"log_total_count\",\n",
        "    \"w_log1p\",\n",
        "    \"w_tf_user\",\n",
        "    \"w_tf_sub\",\n",
        "    \"w_sym_norm\",\n",
        "    \"w_pmi\",\n",
        "    \"w_ppmi\",\n",
        "    \"w_bm25\",\n",
        "    \"w_tfidf_user\",\n",
        "]\n",
        "enriched.head()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantile summaries\n",
        "quant_df = summarize_quantiles(enriched, metrics, quantiles=[0.5, 0.9, 0.99, 0.999])\n",
        "quant_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix and heatmap\n",
        "corr = compute_correlations(enriched, metrics)\n",
        "corr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.heatmap(corr, vmin=-1, vmax=1, cmap=\"vlag\", square=True, cbar=True)\n",
        "plt.title(\"Correlation of normalization schemes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper: histogram plotting\n",
        "\n",
        "def hist(series: pd.Series, title: str, bins: int = 200) -> None:\n",
        "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    use_log = s.max() / (s.min() + 1e-12) > 1e3\n",
        "    sns.histplot(s, bins=bins, ax=ax, stat=\"density\")\n",
        "    ax.set_title(title)\n",
        "    if use_log:\n",
        "        ax.set_xscale(\"log\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "sample = enriched.sample(n=min(200_000, enriched.height), with_replacement=False, shuffle=True).to_pandas()\n",
        "\n",
        "for c in [\"log_total_count\", \"w_sym_norm\", \"w_ppmi\", \"w_bm25\", \"w_tfidf_user\", \"w_tf_user\"]:\n",
        "    hist(sample[c], c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Degree and marginal distributions\n",
        "user_pdf = user_stats.select([\"user_total\", \"user_degree\"]).to_pandas()\n",
        "sub_pdf = sub_stats.select([\"sub_total\", \"sub_degree\"]).to_pandas()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 7))\n",
        "for ax, s, title in [\n",
        "    (axes[0,0], user_pdf[\"user_total\"], \"User total interactions\"),\n",
        "    (axes[0,1], user_pdf[\"user_degree\"], \"User degree (#subreddits)\"),\n",
        "    (axes[1,0], sub_pdf[\"sub_total\"], \"Subreddit total interactions\"),\n",
        "    (axes[1,1], sub_pdf[\"sub_degree\"], \"Subreddit degree (#users)\"),\n",
        "]:\n",
        "    sns.histplot(s.replace([np.inf, -np.inf], np.nan).dropna(), ax=ax, bins=200)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "Based on typical Reddit-like heavy-tailed activity and popularity, consider these for training:\n",
        "\n",
        "- Symmetric normalization: `w_sym_norm = c_ij / sqrt(r_i * c_j)`\n",
        "  - Discounts both power users and popular subreddits; aligns with normalized adjacency.\n",
        "- Positive PMI: `w_ppmi = max(0, log(c_ij * S / (r_i * c_j)))`\n",
        "  - Highlights associations stronger than popularity baseline; optionally clip or apply sqrt.\n",
        "- BM25 per user (k1=1.2, b=0.75): `w_bm25`\n",
        "  - Caps gains from repeated interactions and length-normalizes by user activity.\n",
        "- TF-IDF per user: `w_tfidf_user`\n",
        "  - Simple baseline that downweights globally popular subreddits.\n",
        "\n",
        "Keep `log1p(total)` for ablations. If using GNN message passing, symmetric normalization is a natural fit.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
